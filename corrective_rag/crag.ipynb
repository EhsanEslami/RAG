{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from dotenv import load_dotenv\n",
    "from langchain import hub\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.prompts.chat import (\n",
    "    HumanMessagePromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    ")\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from operator import itemgetter\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.load import dumps, loads\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "from colorama import Fore\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "LANGUAGE_MODEL = \"gpt-4 turbo\"\n",
    "llm = ChatOpenAI()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### INDEXING ####\n",
    "urls = [\n",
    "    \"http://blog.langchain.dev/deconstructing-rag\",\n",
    "    \"https://blog.langchain.dev/enhancing-rag-based-applications-accuracy-by-constructing-and-leveraging-knowledge-graphs/\",\n",
    "    \"https://blog.langchain.dev/graph-based-metadata-filtering-for-improving-vector-search-in-rag-applications/\",\n",
    "]\n",
    "\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=250, chunk_overlap=0\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=doc_splits,\n",
    "    collection_name=\"rag-chroma\",\n",
    "    embedding=OpenAIEmbeddings(),\n",
    ")\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Retrieval Grader : Retrieval Evaluator ####\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(description=\"Documents are relevant to the question, 'yes' or 'no'\")\n",
    "\n",
    "    def get_score(self) -> str:\n",
    "        \"\"\"Return the binary score as a string.\"\"\"\n",
    "        return self.binary_score\n",
    "\n",
    "\n",
    "def get_score(self) -> str:\n",
    "    \"\"\"Return the binary score as a string.\"\"\"\n",
    "    return self.binary_score\n",
    "\n",
    "# LLM with function call \n",
    "structured_llm_grader = llm.with_structured_output(GradeDocuments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt \n",
    "system_template = \"\"\"You are an evaluator determining the relevance of a retrieved {documents} to a user's query {question}.If the document contains keyword(s) or semantic meaning related to the question, mark it as relevant.Assign a binary score of 'yes' or 'no' to indicate the document's relevance to the question.\"\"\"\n",
    "\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(\n",
    "    input_variables=[\"documents\", \"question\"],\n",
    "    template=\"{question}\",\n",
    ")\n",
    "grader_prompt = ChatPromptTemplate.from_messages(\n",
    "    [system_message_prompt, human_message_prompt]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question Re-writer - Knowledge Refinement ####\n",
    "# Prompt \n",
    "prompt_template = \"\"\"Given a user input {question}, re-write or rephrase the question to optimize the query for the model\"\"\"\n",
    "\n",
    "system_prompt = SystemMessagePromptTemplate.from_template(prompt_template)\n",
    "human_prompt = HumanMessagePromptTemplate.from_template(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"{question}\",\n",
    ")\n",
    "re_write_prompt = ChatPromptTemplate.from_messages(\n",
    "    [system_prompt, human_prompt]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Web Search Tool - Knowledge Searching ####\n",
    "web_search_tool = TavilySearchResults(k=3) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/rag/lib/python3.10/site-packages/langchain/hub.py:86: DeprecationWarning: The `langchainhub sdk` is deprecated.\n",
      "Please use the `langsmith sdk` instead:\n",
      "  pip install langsmith\n",
      "Use the `pull_prompt` method.\n",
      "  res_dict = client.pull_repo(owner_repo_commit)\n"
     ]
    }
   ],
   "source": [
    "#### Generate Answer  ####\n",
    "# Prompt\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### RETRIEVAL and GENERATION ####\n",
    "def assess_retrieve_docs(query):\n",
    "    retrieval_grader = grader_prompt | structured_llm_grader | get_score\n",
    "    docs = retriever.get_relevant_documents(query)\n",
    "    doc_txt = docs[1].page_content\n",
    "    binary_score = retrieval_grader.invoke({\"question\": query, \"documents\": doc_txt})\n",
    "    return binary_score, docs\n",
    "\n",
    "\n",
    "def rewrite_query(query):\n",
    "    question_rewriter = re_write_prompt | llm | StrOutputParser()\n",
    "    return question_rewriter.invoke({\"question\": query})\n",
    "\n",
    "def search_web(query):\n",
    "    docs = web_search_tool.invoke({\"query\": query})\n",
    "    web_results = \"\\n\".join([d[\"content\"] for d in docs])\n",
    "    return Document(page_content=web_results)\n",
    "\n",
    "def generate_answer(docs, query):\n",
    "    # Chain\n",
    "    rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "    # Run\n",
    "    return rag_chain.invoke({\"context\": docs, \"question\": query})\n",
    "\n",
    "\n",
    "def query():\n",
    "    \"\"\"Query the model with a question and assess the relevance of retrieved documents.\"\"\"\n",
    "\n",
    "    # query and evaluate\n",
    "    question = \"prompt engineering?\"\n",
    "    binary_score, docs = assess_retrieve_docs(question)\n",
    "    print(\"binary score:\", binary_score)\n",
    "    if binary_score == \"no\":\n",
    "        print(f\"{Fore.MAGENTA}Retrieval is not relevant. Searching the web...{Fore.RESET}\")\n",
    "        docs = search_web(question) \n",
    "    print(f\"{Fore.YELLOW}Retrieval, rewriting and optmize the query...{Fore.RESET}\")  \n",
    "    optimized_query = rewrite_query(question)\n",
    "    return generate_answer(docs, optimized_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/rag/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary score: no\n",
      "\u001b[35mRetrieval is not relevant. Searching the web...\u001b[39m\n",
      "\u001b[33mRetrieval, rewriting and optmize the query...\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Prompt engineering is a new discipline used to develop and optimize prompts for language models in various applications like question answering and arithmetic reasoning. It helps improve the performance of language models by creating specific prompts for different tasks. Prompt engineering is essential in fields like medicine and writing to enhance user experience with AI models.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
